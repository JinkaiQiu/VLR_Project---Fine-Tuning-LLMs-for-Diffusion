{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdBjRnWqLwWP"
      },
      "source": [
        "# Inference notenook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)\n",
        "\n",
        "Disclaimer: the authors do not own any rights for the code or data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfpGaz27IWs",
        "outputId": "ebf43909-76e1-4c4a-e387-3501e9df9c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (4.27.4)\n",
            "Requirement already satisfied: filelock in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
            "Requirement already satisfied: requests in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/8g/x9ncyhjj3wdchhjpvtkcr_640000gp/T/pip-req-build-0abk16z3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/8g/x9ncyhjj3wdchhjpvtkcr_640000gp/T/pip-req-build-0abk16z3\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from clip==1.0) (2023.3.23)\n",
            "Requirement already satisfied: tqdm in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from clip==1.0) (2.2.2)\n",
            "Requirement already satisfied: torchvision in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from clip==1.0) (0.17.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torch->clip==1.0) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torch->clip==1.0) (3.0)\n",
            "Requirement already satisfied: jinja2 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torch->clip==1.0) (2024.2.0)\n",
            "Requirement already satisfied: numpy in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.24.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/ram/miniforge3/envs/vlrfashion/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "#@title Install\n",
        "!pip install transformers\n",
        "! pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "form",
        "id": "OArDkm_24w4L"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "from IPython.display import Image \n",
        "\n",
        "\n",
        "N = type(None)\n",
        "V = np.array\n",
        "ARRAY = np.ndarray\n",
        "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS = Union[Tuple[V, ...], List[V]]\n",
        "VN = Union[V, N]\n",
        "VNS = Union[VS, N]\n",
        "T = torch.Tensor\n",
        "TS = Union[Tuple[T, ...], List[T]]\n",
        "TN = Optional[T]\n",
        "TNS = Union[Tuple[TN, ...], List[TN]]\n",
        "TSN = Optional[TS]\n",
        "TA = Union[T, ARRAY]\n",
        "\n",
        "\n",
        "D = torch.device\n",
        "CPU = torch.device('cpu')\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -> D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f'cuda:{device_id}')\n",
        "\n",
        "\n",
        "CUDA = get_device\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model_path = os.path.join(save_path, 'conceptual_weights.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "cellView": "form",
        "id": "4ClW2ebek8DK"
      },
      "outputs": [],
      "source": [
        "#@title Model\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -> T:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "form",
        "id": "V7xocT3TUgey"
      },
      "outputs": [],
      "source": [
        "#@title Caption prediction\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated = embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens = tokens.unsqueeze(0).to(device)\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            logits = logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated = embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "cellView": "form",
        "id": "7lCgFHSgr_ny"
      },
      "outputs": [],
      "source": [
        "#@title GPU/CPU\n",
        "\n",
        "\n",
        "is_gpu = False #@param {type:\"boolean\"}  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "6bi_2zQ3QD57",
        "outputId": "47afad0d-a76c-4316-8f6d-682dd3e49587"
      },
      "outputs": [],
      "source": [
        "#@title CLIP model + GPT2 tokenizer\n",
        "\n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1eb939f6352f4063808350cc4a97beae",
            "56c9a56fb276438ab3f6b120193deb42",
            "873ac85e2658404ea6b284707824a07d",
            "bcd00181175a4b739afc7a5eb30a235b",
            "2094ef62bd9547fcb133f565818495fa",
            "5a2eb4632f5d477182fcbf0ebcd7eb5b",
            "93c09ca8d2ab40d381163b41c3bd345d",
            "f0585589cf7b49098b0905ed10d96ecf",
            "90bddc0c0d164e93996d0340f45bb9b5",
            "5dc2c242f1594631a573bdbe4ace5a97",
            "7ab34ffb29124fe9a2bf8b2aadf8c514"
          ]
        },
        "id": "glBzYsgIwhwF",
        "outputId": "2d7637a4-1c76-44b1-c0aa-80a308ee0717"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ClipCaptionModel:\n\tMissing key(s) in state_dict: \"gpt.transformer.h.0.attn.bias\", \"gpt.transformer.h.0.attn.masked_bias\", \"gpt.transformer.h.1.attn.bias\", \"gpt.transformer.h.1.attn.masked_bias\", \"gpt.transformer.h.2.attn.bias\", \"gpt.transformer.h.2.attn.masked_bias\", \"gpt.transformer.h.3.attn.bias\", \"gpt.transformer.h.3.attn.masked_bias\", \"gpt.transformer.h.4.attn.bias\", \"gpt.transformer.h.4.attn.masked_bias\", \"gpt.transformer.h.5.attn.bias\", \"gpt.transformer.h.5.attn.masked_bias\", \"gpt.transformer.h.6.attn.bias\", \"gpt.transformer.h.6.attn.masked_bias\", \"gpt.transformer.h.7.attn.bias\", \"gpt.transformer.h.7.attn.masked_bias\", \"gpt.transformer.h.8.attn.bias\", \"gpt.transformer.h.8.attn.masked_bias\", \"gpt.transformer.h.9.attn.bias\", \"gpt.transformer.h.9.attn.masked_bias\", \"gpt.transformer.h.10.attn.bias\", \"gpt.transformer.h.10.attn.masked_bias\", \"gpt.transformer.h.11.attn.bias\", \"gpt.transformer.h.11.attn.masked_bias\", \"clip_project.model.0.weight\", \"clip_project.model.0.bias\", \"clip_project.model.2.weight\", \"clip_project.model.2.bias\". \n\tUnexpected key(s) in state_dict: \"clip_project.prefix_const\", \"clip_project.transformer.layers.0.norm1.weight\", \"clip_project.transformer.layers.0.norm1.bias\", \"clip_project.transformer.layers.0.attn.to_queries.weight\", \"clip_project.transformer.layers.0.attn.to_keys_values.weight\", \"clip_project.transformer.layers.0.attn.project.weight\", \"clip_project.transformer.layers.0.attn.project.bias\", \"clip_project.transformer.layers.0.norm2.weight\", \"clip_project.transformer.layers.0.norm2.bias\", \"clip_project.transformer.layers.0.mlp.fc1.weight\", \"clip_project.transformer.layers.0.mlp.fc1.bias\", \"clip_project.transformer.layers.0.mlp.fc2.weight\", \"clip_project.transformer.layers.0.mlp.fc2.bias\", \"clip_project.transformer.layers.1.norm1.weight\", \"clip_project.transformer.layers.1.norm1.bias\", \"clip_project.transformer.layers.1.attn.to_queries.weight\", \"clip_project.transformer.layers.1.attn.to_keys_values.weight\", \"clip_project.transformer.layers.1.attn.project.weight\", \"clip_project.transformer.layers.1.attn.project.bias\", \"clip_project.transformer.layers.1.norm2.weight\", \"clip_project.transformer.layers.1.norm2.bias\", \"clip_project.transformer.layers.1.mlp.fc1.weight\", \"clip_project.transformer.layers.1.mlp.fc1.bias\", \"clip_project.transformer.layers.1.mlp.fc2.weight\", \"clip_project.transformer.layers.1.mlp.fc2.bias\", \"clip_project.transformer.layers.2.norm1.weight\", \"clip_project.transformer.layers.2.norm1.bias\", \"clip_project.transformer.layers.2.attn.to_queries.weight\", \"clip_project.transformer.layers.2.attn.to_keys_values.weight\", \"clip_project.transformer.layers.2.attn.project.weight\", \"clip_project.transformer.layers.2.attn.project.bias\", \"clip_project.transformer.layers.2.norm2.weight\", \"clip_project.transformer.layers.2.norm2.bias\", \"clip_project.transformer.layers.2.mlp.fc1.weight\", \"clip_project.transformer.layers.2.mlp.fc1.bias\", \"clip_project.transformer.layers.2.mlp.fc2.weight\", \"clip_project.transformer.layers.2.mlp.fc2.bias\", \"clip_project.transformer.layers.3.norm1.weight\", \"clip_project.transformer.layers.3.norm1.bias\", \"clip_project.transformer.layers.3.attn.to_queries.weight\", \"clip_project.transformer.layers.3.attn.to_keys_values.weight\", \"clip_project.transformer.layers.3.attn.project.weight\", \"clip_project.transformer.layers.3.attn.project.bias\", \"clip_project.transformer.layers.3.norm2.weight\", \"clip_project.transformer.layers.3.norm2.bias\", \"clip_project.transformer.layers.3.mlp.fc1.weight\", \"clip_project.transformer.layers.3.mlp.fc1.bias\", \"clip_project.transformer.layers.3.mlp.fc2.weight\", \"clip_project.transformer.layers.3.mlp.fc2.bias\", \"clip_project.transformer.layers.4.norm1.weight\", \"clip_project.transformer.layers.4.norm1.bias\", \"clip_project.transformer.layers.4.attn.to_queries.weight\", \"clip_project.transformer.layers.4.attn.to_keys_values.weight\", \"clip_project.transformer.layers.4.attn.project.weight\", \"clip_project.transformer.layers.4.attn.project.bias\", \"clip_project.transformer.layers.4.norm2.weight\", \"clip_project.transformer.layers.4.norm2.bias\", \"clip_project.transformer.layers.4.mlp.fc1.weight\", \"clip_project.transformer.layers.4.mlp.fc1.bias\", \"clip_project.transformer.layers.4.mlp.fc2.weight\", \"clip_project.transformer.layers.4.mlp.fc2.bias\", \"clip_project.transformer.layers.5.norm1.weight\", \"clip_project.transformer.layers.5.norm1.bias\", \"clip_project.transformer.layers.5.attn.to_queries.weight\", \"clip_project.transformer.layers.5.attn.to_keys_values.weight\", \"clip_project.transformer.layers.5.attn.project.weight\", \"clip_project.transformer.layers.5.attn.project.bias\", \"clip_project.transformer.layers.5.norm2.weight\", \"clip_project.transformer.layers.5.norm2.bias\", \"clip_project.transformer.layers.5.mlp.fc1.weight\", \"clip_project.transformer.layers.5.mlp.fc1.bias\", \"clip_project.transformer.layers.5.mlp.fc2.weight\", \"clip_project.transformer.layers.5.mlp.fc2.bias\", \"clip_project.transformer.layers.6.norm1.weight\", \"clip_project.transformer.layers.6.norm1.bias\", \"clip_project.transformer.layers.6.attn.to_queries.weight\", \"clip_project.transformer.layers.6.attn.to_keys_values.weight\", \"clip_project.transformer.layers.6.attn.project.weight\", \"clip_project.transformer.layers.6.attn.project.bias\", \"clip_project.transformer.layers.6.norm2.weight\", \"clip_project.transformer.layers.6.norm2.bias\", \"clip_project.transformer.layers.6.mlp.fc1.weight\", \"clip_project.transformer.layers.6.mlp.fc1.bias\", \"clip_project.transformer.layers.6.mlp.fc2.weight\", \"clip_project.transformer.layers.6.mlp.fc2.bias\", \"clip_project.transformer.layers.7.norm1.weight\", \"clip_project.transformer.layers.7.norm1.bias\", \"clip_project.transformer.layers.7.attn.to_queries.weight\", \"clip_project.transformer.layers.7.attn.to_keys_values.weight\", \"clip_project.transformer.layers.7.attn.project.weight\", \"clip_project.transformer.layers.7.attn.project.bias\", \"clip_project.transformer.layers.7.norm2.weight\", \"clip_project.transformer.layers.7.norm2.bias\", \"clip_project.transformer.layers.7.mlp.fc1.weight\", \"clip_project.transformer.layers.7.mlp.fc1.bias\", \"clip_project.transformer.layers.7.mlp.fc2.weight\", \"clip_project.transformer.layers.7.mlp.fc2.bias\", \"clip_project.linear.weight\", \"clip_project.linear.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m prefix_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m ClipCaptionModel(prefix_length)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCPU\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m CUDA(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m is_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniforge3/envs/vlrfashion/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClipCaptionModel:\n\tMissing key(s) in state_dict: \"gpt.transformer.h.0.attn.bias\", \"gpt.transformer.h.0.attn.masked_bias\", \"gpt.transformer.h.1.attn.bias\", \"gpt.transformer.h.1.attn.masked_bias\", \"gpt.transformer.h.2.attn.bias\", \"gpt.transformer.h.2.attn.masked_bias\", \"gpt.transformer.h.3.attn.bias\", \"gpt.transformer.h.3.attn.masked_bias\", \"gpt.transformer.h.4.attn.bias\", \"gpt.transformer.h.4.attn.masked_bias\", \"gpt.transformer.h.5.attn.bias\", \"gpt.transformer.h.5.attn.masked_bias\", \"gpt.transformer.h.6.attn.bias\", \"gpt.transformer.h.6.attn.masked_bias\", \"gpt.transformer.h.7.attn.bias\", \"gpt.transformer.h.7.attn.masked_bias\", \"gpt.transformer.h.8.attn.bias\", \"gpt.transformer.h.8.attn.masked_bias\", \"gpt.transformer.h.9.attn.bias\", \"gpt.transformer.h.9.attn.masked_bias\", \"gpt.transformer.h.10.attn.bias\", \"gpt.transformer.h.10.attn.masked_bias\", \"gpt.transformer.h.11.attn.bias\", \"gpt.transformer.h.11.attn.masked_bias\", \"clip_project.model.0.weight\", \"clip_project.model.0.bias\", \"clip_project.model.2.weight\", \"clip_project.model.2.bias\". \n\tUnexpected key(s) in state_dict: \"clip_project.prefix_const\", \"clip_project.transformer.layers.0.norm1.weight\", \"clip_project.transformer.layers.0.norm1.bias\", \"clip_project.transformer.layers.0.attn.to_queries.weight\", \"clip_project.transformer.layers.0.attn.to_keys_values.weight\", \"clip_project.transformer.layers.0.attn.project.weight\", \"clip_project.transformer.layers.0.attn.project.bias\", \"clip_project.transformer.layers.0.norm2.weight\", \"clip_project.transformer.layers.0.norm2.bias\", \"clip_project.transformer.layers.0.mlp.fc1.weight\", \"clip_project.transformer.layers.0.mlp.fc1.bias\", \"clip_project.transformer.layers.0.mlp.fc2.weight\", \"clip_project.transformer.layers.0.mlp.fc2.bias\", \"clip_project.transformer.layers.1.norm1.weight\", \"clip_project.transformer.layers.1.norm1.bias\", \"clip_project.transformer.layers.1.attn.to_queries.weight\", \"clip_project.transformer.layers.1.attn.to_keys_values.weight\", \"clip_project.transformer.layers.1.attn.project.weight\", \"clip_project.transformer.layers.1.attn.project.bias\", \"clip_project.transformer.layers.1.norm2.weight\", \"clip_project.transformer.layers.1.norm2.bias\", \"clip_project.transformer.layers.1.mlp.fc1.weight\", \"clip_project.transformer.layers.1.mlp.fc1.bias\", \"clip_project.transformer.layers.1.mlp.fc2.weight\", \"clip_project.transformer.layers.1.mlp.fc2.bias\", \"clip_project.transformer.layers.2.norm1.weight\", \"clip_project.transformer.layers.2.norm1.bias\", \"clip_project.transformer.layers.2.attn.to_queries.weight\", \"clip_project.transformer.layers.2.attn.to_keys_values.weight\", \"clip_project.transformer.layers.2.attn.project.weight\", \"clip_project.transformer.layers.2.attn.project.bias\", \"clip_project.transformer.layers.2.norm2.weight\", \"clip_project.transformer.layers.2.norm2.bias\", \"clip_project.transformer.layers.2.mlp.fc1.weight\", \"clip_project.transformer.layers.2.mlp.fc1.bias\", \"clip_project.transformer.layers.2.mlp.fc2.weight\", \"clip_project.transformer.layers.2.mlp.fc2.bias\", \"clip_project.transformer.layers.3.norm1.weight\", \"clip_project.transformer.layers.3.norm1.bias\", \"clip_project.transformer.layers.3.attn.to_queries.weight\", \"clip_project.transformer.layers.3.attn.to_keys_values.weight\", \"clip_project.transformer.layers.3.attn.project.weight\", \"clip_project.transformer.layers.3.attn.project.bias\", \"clip_project.transformer.layers.3.norm2.weight\", \"clip_project.transformer.layers.3.norm2.bias\", \"clip_project.transformer.layers.3.mlp.fc1.weight\", \"clip_project.transformer.layers.3.mlp.fc1.bias\", \"clip_project.transformer.layers.3.mlp.fc2.weight\", \"clip_project.transformer.layers.3.mlp.fc2.bias\", \"clip_project.transformer.layers.4.norm1.weight\", \"clip_project.transformer.layers.4.norm1.bias\", \"clip_project.transformer.layers.4.attn.to_queries.weight\", \"clip_project.transformer.layers.4.attn.to_keys_values.weight\", \"clip_project.transformer.layers.4.attn.project.weight\", \"clip_project.transformer.layers.4.attn.project.bias\", \"clip_project.transformer.layers.4.norm2.weight\", \"clip_project.transformer.layers.4.norm2.bias\", \"clip_project.transformer.layers.4.mlp.fc1.weight\", \"clip_project.transformer.layers.4.mlp.fc1.bias\", \"clip_project.transformer.layers.4.mlp.fc2.weight\", \"clip_project.transformer.layers.4.mlp.fc2.bias\", \"clip_project.transformer.layers.5.norm1.weight\", \"clip_project.transformer.layers.5.norm1.bias\", \"clip_project.transformer.layers.5.attn.to_queries.weight\", \"clip_project.transformer.layers.5.attn.to_keys_values.weight\", \"clip_project.transformer.layers.5.attn.project.weight\", \"clip_project.transformer.layers.5.attn.project.bias\", \"clip_project.transformer.layers.5.norm2.weight\", \"clip_project.transformer.layers.5.norm2.bias\", \"clip_project.transformer.layers.5.mlp.fc1.weight\", \"clip_project.transformer.layers.5.mlp.fc1.bias\", \"clip_project.transformer.layers.5.mlp.fc2.weight\", \"clip_project.transformer.layers.5.mlp.fc2.bias\", \"clip_project.transformer.layers.6.norm1.weight\", \"clip_project.transformer.layers.6.norm1.bias\", \"clip_project.transformer.layers.6.attn.to_queries.weight\", \"clip_project.transformer.layers.6.attn.to_keys_values.weight\", \"clip_project.transformer.layers.6.attn.project.weight\", \"clip_project.transformer.layers.6.attn.project.bias\", \"clip_project.transformer.layers.6.norm2.weight\", \"clip_project.transformer.layers.6.norm2.bias\", \"clip_project.transformer.layers.6.mlp.fc1.weight\", \"clip_project.transformer.layers.6.mlp.fc1.bias\", \"clip_project.transformer.layers.6.mlp.fc2.weight\", \"clip_project.transformer.layers.6.mlp.fc2.bias\", \"clip_project.transformer.layers.7.norm1.weight\", \"clip_project.transformer.layers.7.norm1.bias\", \"clip_project.transformer.layers.7.attn.to_queries.weight\", \"clip_project.transformer.layers.7.attn.to_keys_values.weight\", \"clip_project.transformer.layers.7.attn.project.weight\", \"clip_project.transformer.layers.7.attn.project.bias\", \"clip_project.transformer.layers.7.norm2.weight\", \"clip_project.transformer.layers.7.norm2.bias\", \"clip_project.transformer.layers.7.mlp.fc1.weight\", \"clip_project.transformer.layers.7.mlp.fc1.bias\", \"clip_project.transformer.layers.7.mlp.fc2.weight\", \"clip_project.transformer.layers.7.mlp.fc2.bias\", \"clip_project.linear.weight\", \"clip_project.linear.bias\". "
          ]
        }
      ],
      "source": [
        "#@title Load model weights\n",
        "\n",
        "\n",
        "prefix_length = 10\n",
        "\n",
        "model = ClipCaptionModel(prefix_length)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
        "\n",
        "model = model.eval() \n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "\n",
        "fclip = FashionCLIP('fashion-clip')\n",
        "\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fashion_clip_process(pil_image):\n",
        "    image_embeddings = fclip.encode_images([pil_image], batch_size=1)\n",
        "    image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)\n",
        "    image_embeddings = torch.tensor(image_embeddings).to(device)\n",
        "    return image_embeddings\n",
        "\n",
        "def vit_clip_process(pil_image): \n",
        "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "    return prefix\n",
        "\n",
        "USE_FASHION_CLIP = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_ = \"0108775044.jpg\"\n",
        "images_path = os.path.join(os.path.dirname(current_directory), \"fashion_data\")\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "UPLOADED_FILE = os.path.join(images_path, name_)\n",
        "\n",
        "use_beam_search = False #@param {type:\"boolean\"}  \n",
        "\n",
        "image = io.imread(UPLOADED_FILE)\n",
        "pil_image = PIL.Image.fromarray(image)\n",
        "display(pil_image)\n",
        "\n",
        "image_embeddings = fashion_clip_process(pil_image) if USE_FASHION_CLIP else vit_clip_process(pil_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "white tank top with a white trim.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    prefix_embed = model.clip_project(image_embeddings).reshape(1, prefix_length, -1)\n",
        "\n",
        "if use_beam_search:\n",
        "    generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
        "else:\n",
        "    generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print(generated_text_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyVkuZ07llSC"
      },
      "source": [
        "Conceptual captions examples:\n",
        "https://drive.google.com/file/d/1mzH3b0LQrGEWjEva4hI6HE_fIYRIgtBT/view?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "clip_prefix_captioning_inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1eb939f6352f4063808350cc4a97beae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_873ac85e2658404ea6b284707824a07d",
              "IPY_MODEL_bcd00181175a4b739afc7a5eb30a235b",
              "IPY_MODEL_2094ef62bd9547fcb133f565818495fa"
            ],
            "layout": "IPY_MODEL_56c9a56fb276438ab3f6b120193deb42"
          }
        },
        "2094ef62bd9547fcb133f565818495fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab34ffb29124fe9a2bf8b2aadf8c514",
            "placeholder": "​",
            "style": "IPY_MODEL_5dc2c242f1594631a573bdbe4ace5a97",
            "value": " 523M/523M [00:20&lt;00:00, 25.5MB/s]"
          }
        },
        "56c9a56fb276438ab3f6b120193deb42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2eb4632f5d477182fcbf0ebcd7eb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dc2c242f1594631a573bdbe4ace5a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ab34ffb29124fe9a2bf8b2aadf8c514": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "873ac85e2658404ea6b284707824a07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c09ca8d2ab40d381163b41c3bd345d",
            "placeholder": "​",
            "style": "IPY_MODEL_5a2eb4632f5d477182fcbf0ebcd7eb5b",
            "value": "Downloading: 100%"
          }
        },
        "90bddc0c0d164e93996d0340f45bb9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c09ca8d2ab40d381163b41c3bd345d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd00181175a4b739afc7a5eb30a235b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90bddc0c0d164e93996d0340f45bb9b5",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0585589cf7b49098b0905ed10d96ecf",
            "value": 548118077
          }
        },
        "f0585589cf7b49098b0905ed10d96ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
